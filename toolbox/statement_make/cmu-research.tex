\documentclass[10pt]{article} % Use the res.cls style, the font size can be changed to 11pt or 12pt here

\usepackage{helvet} % Default font is the helvetica postscript font
\usepackage{hyperref}
\usepackage[a4paper,left=1in,top=1in,right=1in,bottom=1in]{geometry}
\usepackage{fancyhdr}

\pagestyle{fancy}
\lhead{Clement Fung}
\rhead{User ID: clement.y.fung@gmail.com}

\begin{document}

\begin{center}
{\large \bf Research Experience Statement for Carnegie Mellon Societal Computing PhD Application}
\end{center}

I was first introduced to research during my masters degree at the University of British Columbia (UBC), where I have been a research assistant in the Networks, Systems and Security (NSS) Lab for the past 2 years. Throughout my masters degree, I was supervised by Professor Ivan Beschastnikh, whose expertise spans from distributed systems to systems security. Together, we completed three different projects in the area of secure and private multi-party machine learning:
\begin{itemize}
\item Biscotti\footnote{ArXiv, November 2018. \url{https://arxiv.org/abs/1808.04866}}: A private and secure distributed ledger for peer to peer machine learning
\item FoolsGold\footnote{ArXiv, August 2018. \url{https://arxiv.org/abs/1808.04866}}: A protocol for detecting and mitigating sybil-based poisoning attacks on federated learning
\item TorMentor\footnote{ArXiv, November 2018. \url{https://arxiv.org/abs/1808.04866}}: A system and protocol for private, secure machine learning over an anonymous network
\end{itemize}

For each of these projects, the central theme was: how can we modify distributed multi-party machine learning systems in ways that protect the privacy of their users while maintaining the integrity of the learned model? \\

One solution to providing more privacy and security to distributed multi-party machine learning is to eliminate the centralization present in modern architectures such as federated learning. I worked on developing an alternative peer-to-peer solution that does not rely on a centralized process to store and coordinate the training process, called Biscotti. 
%
The peer-to-peer setting requires a novel threat model in ML, and Biscotti adapts elements of distributed ledgers, such as proof of stake, block verification, and cryptographic commitments to ensure a private and secure mechanism for peer-to-peer machine learning through a ledger-based structure. This system was built and designed over a 6 month period in collaboration with another graduate student, in which I focused on developing the distributed machine learning algorithms and implemented machine learning attacks and defenses, while they worked on the consensus and cryptographic elements of the project. This was my first experience in dealing with the teamwork required in successfully sharing the duties of system development, evaluation and paper writing in co-authoring a top tier conference submission. Ultimately, we designed and implemented a system that enables peer-to-peer, private, secure machine learning at scales up to 100 peers, matching model convergence results from federated learning. The system provides state-of-the-art privacy through differential privacy and secure aggregation while using blockchain primitives to prevent sybil attacks.\\

Sybil attacks are also relevant to federated multi-party learning systems; I am highly interested in defending these systems and thus developed FoolsGold, a mechanism for protecting federated learning systems from sybil-based targeted poisoning attacks. Prior work in this space relies on assumptions of a bounded proportion of attackers, and relies on direct analysis of the training data, which cannot be applied to privacy-preserving machine learning. In FoolsGold, I identified that the similarity of gradients between clients was an effective tool for detecting sybils and designed a penalization function for thwarting targeted poisoning attacks. Unlike prior defenses, this mechanism can actively resist an attack from a system with 99\% sybils and unlike prior work, does not rely on observation of client training data, which makes it suitable for federated learning systems. This work is currently in submission at the 2019 IEEE European Symposium on Security and Privacy. \\

In TorMentor, I augmented stronger defenses onto federated learning by using anonymous onion routers as the communication medium in distributed learning. Through anonymous communication, I defined a new learning paradigm called brokered learning, in which data providers and model curators do not need to directly communicate with each other; instead, they coordinate with third-party brokers to perform distributed machine learning. In doing so, model definers are no longer the central authority on the training process: Unlike prior work which requires trusting the model curator to provide privacy, TorMentor gives more control to clients and performs secure and anonymous machine learning in a democratic fashion: providing privacy and control to data providers while concurrently attempting to attain optimal model performance. \\

Combined, these projects were submitted to 5 different conferences, and I have endured the process of paper rejection and revision with each of them. This involves the tricky process of navigating and addressing reviewer feedback, only to come back with a stronger paper submission. \\

In addition to my submission experiences, I have also given two talks at external venues: once as a visiting researcher at the University of Toronto, and once at a local cybersecurity summit in Vancouver. I have also been a subreviewer for my supervisor, Ivan Beschastnikh, for submitted work to the IEEE International Symposium of Software Reliability Engineering (ISSRE 2017). All of these experiences have enhanced my research experience at UBC, and simultaneously taught me the importance of effective communication and presentation of one's work as a researcher. 

\end{document}
